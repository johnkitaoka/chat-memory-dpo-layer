[2024-06-25 09:15:00] USER: I'm building a Python API that needs to handle file uploads. What's the best approach for handling large files efficiently?
[2024-06-25 09:15:30] ASSISTANT: For handling large file uploads efficiently in Python APIs, here are the key approaches:

## Streaming Upload with FastAPI/Flask

```python
from fastapi import FastAPI, File, UploadFile
import aiofiles

app = FastAPI()

@app.post("/upload/")
async def upload_file(file: UploadFile = File(...)):
    async with aiofiles.open(f"uploads/{file.filename}", 'wb') as f:
        while chunk := await file.read(1024 * 1024):  # 1MB chunks
            await f.write(chunk)
    return {"filename": file.filename}
```

## Key Strategies:
1. **Chunked Processing**: Read files in chunks to avoid memory issues
2. **Async I/O**: Use async operations for better concurrency
3. **Direct Storage**: Stream directly to disk/cloud storage
4. **Size Limits**: Set reasonable file size limits
5. **Progress Tracking**: Implement upload progress for large files

Would you like me to show you cloud storage integration or progress tracking implementation?
[2024-06-25 09:17:00] USER: Yes, show me how to integrate with AWS S3 for direct uploads. I want to avoid storing files locally.
[2024-06-25 09:17:45] ASSISTANT: Here's how to implement direct S3 uploads without local storage:

## Direct S3 Upload with Streaming

```python
import boto3
from fastapi import FastAPI, File, UploadFile
from botocore.exceptions import ClientError

app = FastAPI()
s3_client = boto3.client('s3')

@app.post("/upload-to-s3/")
async def upload_to_s3(file: UploadFile = File(...)):
    try:
        # Stream directly to S3
        s3_client.upload_fileobj(
            file.file,
            'your-bucket-name',
            f"uploads/{file.filename}",
            ExtraArgs={'ContentType': file.content_type}
        )
        
        return {
            "message": "Upload successful",
            "filename": file.filename,
            "s3_key": f"uploads/{file.filename}"
        }
    except ClientError as e:
        return {"error": f"Upload failed: {str(e)}"}
```

## Presigned URLs (Recommended for Large Files)

```python
@app.get("/get-upload-url/")
async def get_upload_url(filename: str, content_type: str):
    try:
        presigned_url = s3_client.generate_presigned_url(
            'put_object',
            Params={
                'Bucket': 'your-bucket-name',
                'Key': f"uploads/{filename}",
                'ContentType': content_type
            },
            ExpiresIn=3600  # 1 hour
        )
        return {"upload_url": presigned_url}
    except ClientError as e:
        return {"error": str(e)}
```

## Benefits of Presigned URLs:
- Client uploads directly to S3
- Reduces server load and bandwidth
- Better for large files
- Built-in progress tracking in browsers

Need help with multipart uploads for files >5GB?
[2024-06-25 09:19:30] USER: Perfect! Can you also show me how to add progress tracking on the client side?
[2024-06-25 09:20:15] ASSISTANT: Here's how to implement client-side progress tracking for S3 uploads:

## JavaScript Client with Progress

```javascript
async function uploadWithProgress(file, uploadUrl) {
    return new Promise((resolve, reject) => {
        const xhr = new XMLHttpRequest();
        
        // Progress tracking
        xhr.upload.addEventListener('progress', (e) => {
            if (e.lengthComputable) {
                const percentComplete = (e.loaded / e.total) * 100;
                updateProgressBar(percentComplete);
                console.log(`Upload progress: ${percentComplete.toFixed(2)}%`);
            }
        });
        
        xhr.addEventListener('load', () => {
            if (xhr.status === 200) {
                resolve(xhr.response);
            } else {
                reject(new Error(`Upload failed: ${xhr.status}`));
            }
        });
        
        xhr.addEventListener('error', () => {
            reject(new Error('Upload failed'));
        });
        
        xhr.open('PUT', uploadUrl);
        xhr.setRequestHeader('Content-Type', file.type);
        xhr.send(file);
    });
}

function updateProgressBar(percent) {
    const progressBar = document.getElementById('progress-bar');
    progressBar.style.width = `${percent}%`;
    progressBar.textContent = `${Math.round(percent)}%`;
}
```

## React Hook for Upload Progress

```jsx
import { useState } from 'react';

function useFileUpload() {
    const [progress, setProgress] = useState(0);
    const [uploading, setUploading] = useState(false);
    
    const uploadFile = async (file) => {
        setUploading(true);
        setProgress(0);
        
        try {
            // Get presigned URL
            const response = await fetch(`/get-upload-url/?filename=${file.name}&content_type=${file.type}`);
            const { upload_url } = await response.json();
            
            // Upload with progress
            await uploadWithProgress(file, upload_url, setProgress);
            
            setUploading(false);
            return { success: true };
        } catch (error) {
            setUploading(false);
            return { success: false, error: error.message };
        }
    };
    
    return { uploadFile, progress, uploading };
}
```

## HTML Progress UI

```html
<div class="upload-container">
    <input type="file" id="file-input" />
    <button onclick="handleUpload()">Upload</button>
    <div class="progress-container">
        <div id="progress-bar" class="progress-bar">0%</div>
    </div>
</div>
```

This gives you real-time progress feedback and a clean user experience. Want me to show you how to handle upload cancellation or retry logic?
[2024-06-25 09:22:00] USER: Thanks! This is exactly what I needed. One more question - how do I handle upload failures and implement retry logic?
[2024-06-25 09:22:45] ASSISTANT: Here's a robust retry system for upload failures:

## Retry Logic with Exponential Backoff

```javascript
class UploadManager {
    constructor(maxRetries = 3, baseDelay = 1000) {
        this.maxRetries = maxRetries;
        this.baseDelay = baseDelay;
    }
    
    async uploadWithRetry(file, uploadUrl, onProgress) {
        let attempt = 0;
        
        while (attempt <= this.maxRetries) {
            try {
                return await this.attemptUpload(file, uploadUrl, onProgress, attempt);
            } catch (error) {
                attempt++;
                
                if (attempt > this.maxRetries) {
                    throw new Error(`Upload failed after ${this.maxRetries} attempts: ${error.message}`);
                }
                
                // Exponential backoff: 1s, 2s, 4s, 8s...
                const delay = this.baseDelay * Math.pow(2, attempt - 1);
                console.log(`Upload attempt ${attempt} failed, retrying in ${delay}ms...`);
                
                await this.sleep(delay);
            }
        }
    }
    
    async attemptUpload(file, uploadUrl, onProgress, attempt) {
        return new Promise((resolve, reject) => {
            const xhr = new XMLHttpRequest();
            let lastLoaded = 0;
            
            xhr.upload.addEventListener('progress', (e) => {
                if (e.lengthComputable) {
                    const percentComplete = (e.loaded / e.total) * 100;
                    onProgress(percentComplete, attempt);
                    lastLoaded = e.loaded;
                }
            });
            
            xhr.addEventListener('load', () => {
                if (xhr.status === 200) {
                    resolve(xhr.response);
                } else {
                    reject(new Error(`HTTP ${xhr.status}: ${xhr.statusText}`));
                }
            });
            
            xhr.addEventListener('error', () => {
                reject(new Error('Network error'));
            });
            
            xhr.addEventListener('timeout', () => {
                reject(new Error('Upload timeout'));
            });
            
            xhr.timeout = 30000; // 30 second timeout
            xhr.open('PUT', uploadUrl);
            xhr.setRequestHeader('Content-Type', file.type);
            xhr.send(file);
        });
    }
    
    sleep(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
}
```

## Usage with Cancel Support

```javascript
class CancellableUpload {
    constructor() {
        this.uploadManager = new UploadManager();
        this.currentXhr = null;
        this.cancelled = false;
    }
    
    async upload(file, onProgress, onStatusChange) {
        this.cancelled = false;
        
        try {
            onStatusChange('Getting upload URL...');
            const response = await fetch(`/get-upload-url/?filename=${file.name}&content_type=${file.type}`);
            const { upload_url } = await response.json();
            
            onStatusChange('Uploading...');
            await this.uploadManager.uploadWithRetry(
                file, 
                upload_url, 
                (progress, attempt) => {
                    if (this.cancelled) throw new Error('Upload cancelled');
                    onProgress(progress, attempt);
                }
            );
            
            onStatusChange('Upload complete!');
            return { success: true };
            
        } catch (error) {
            if (this.cancelled) {
                onStatusChange('Upload cancelled');
                return { success: false, cancelled: true };
            } else {
                onStatusChange(`Upload failed: ${error.message}`);
                return { success: false, error: error.message };
            }
        }
    }
    
    cancel() {
        this.cancelled = true;
        if (this.currentXhr) {
            this.currentXhr.abort();
        }
    }
}
```

## React Component Example

```jsx
function FileUploader() {
    const [progress, setProgress] = useState(0);
    const [status, setStatus] = useState('');
    const [uploading, setUploading] = useState(false);
    const uploadRef = useRef(null);
    
    const handleUpload = async (file) => {
        setUploading(true);
        uploadRef.current = new CancellableUpload();
        
        const result = await uploadRef.current.upload(
            file,
            (progress, attempt) => {
                setProgress(progress);
                if (attempt > 0) {
                    setStatus(`Attempt ${attempt + 1}: ${progress.toFixed(1)}%`);
                } else {
                    setStatus(`${progress.toFixed(1)}%`);
                }
            },
            setStatus
        );
        
        setUploading(false);
        return result;
    };
    
    const handleCancel = () => {
        if (uploadRef.current) {
            uploadRef.current.cancel();
        }
    };
    
    return (
        <div>
            <input type="file" onChange={(e) => handleUpload(e.target.files[0])} />
            {uploading && (
                <div>
                    <div className="progress-bar" style={{width: `${progress}%`}}></div>
                    <div>{status}</div>
                    <button onClick={handleCancel}>Cancel</button>
                </div>
            )}
        </div>
    );
}
```

This system handles network failures, timeouts, and provides user control over uploads. The exponential backoff prevents overwhelming the server during issues.
[2024-06-25 09:25:30] USER: Excellent! This covers everything I need. I'll implement this approach for my file upload system.
[2024-06-25 09:25:45] ASSISTANT: Great! You now have a complete, production-ready file upload system. Here's a quick summary of what you're implementing:

## Implementation Checklist:
- ✅ Direct S3 uploads with presigned URLs
- ✅ Client-side progress tracking
- ✅ Retry logic with exponential backoff
- ✅ Upload cancellation support
- ✅ Error handling and user feedback

## Next Steps:
1. Set up your S3 bucket with proper CORS configuration
2. Configure AWS credentials in your backend
3. Test with various file sizes and network conditions
4. Consider adding file type validation and virus scanning

## Additional Considerations:
- **Security**: Validate file types and sizes on both client and server
- **Monitoring**: Add logging for upload metrics and failures
- **Scaling**: Consider CloudFront for global upload acceleration

Feel free to reach out if you run into any issues during implementation or need help with the S3 CORS configuration!
[2024-06-25 09:26:00] USER: Perfect, thanks for the comprehensive solution!
